{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "matplotlib.rcParams['figure.dpi'] = 144"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Miniproject\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stack Overflow is a collaboratively edited question-and-answer site originally focused on programming topics. Because of the variety of features tracked, including a variety of feedback metrics, it allows for some open-ended analysis of user behavior on the site.\n",
    "\n",
    "Stack Exchange (the parent organization) provides an anonymized [data dump](https://archive.org/details/stackexchange), and we'll use Spark to perform data manipulation, analysis, and machine learning on this data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accessing the data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is available and There are three sub-folders, `allUsers`, `allPosts`, and `allVotes`, which contain Gzipped XML with the following format:\n",
    "\n",
    "```\n",
    "<row Body=\"&lt;p&gt;I always validate my web pages, and I recommend you do the same BUT many large company websites DO NOT and cannot validate because the importance of the website looking exactly the same on all systems requires rules to be broken. &lt;/p&gt;&#10;&#10;&lt;p&gt;In general, valid websites help your page look good even on odd configurations (like cell phones) so you should always at least try to make it validate.&lt;/p&gt;&#10;\" CommentCount=\"0\" CreationDate=\"2008-10-12T20:26:29.397\" Id=\"195995\" LastActivityDate=\"2008-10-12T20:26:29.397\" OwnerDisplayName=\"Eric Wendelin\" OwnerUserId=\"25066\" ParentId=\"195973\" PostTypeId=\"2\" Score=\"0\" />\n",
    "```\n",
    "\n",
    "Data from the much smaller stats.stackexchange.com is available in the same format as spark-stats-data. This site, Cross-Validated, will be used below in some instances to avoid working with the full data set for every step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "see = open('stack_exchange_schema.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "see.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can either get the data by running the appropriate S3 commands in the terminal, or by running this block for the smaller stats data set:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data input and parsing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some rows are split across multiple lines; these can be discarded. Incorrectly formatted XML can also be ignored. It is enough to simply skip problematic rows, the loss of data will not significantly impact our results on this large data sets.\n",
    "\n",
    "We will need to handle XML parsing using  `lxml.etree`.\n",
    "\n",
    "The goal should be to have a parsing function that can be applied to the input data to access any XML element desired. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bad XML\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple step to test our parsing function. We create an RDD of Post objects where each Post is a valid row of XML from the Cross-Validated (stats.stackexchange.com) `allPosts` data set.\n",
    "\n",
    "We are going to take several shortcuts to speed up and simplify our computations.  First, our parsing function only attempts to parse rows that start with `  <row` as these denote actual data entries. \n",
    "\n",
    "Next we will return the total number of XML rows that started with ` <row` that were subsequently **rejected** during your processing.\n",
    "\n",
    "Note that this cleaned data set will be used for future parts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lxml import etree\n",
    "import os\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext(\"local[*]\", \"temp\")\n",
    "\n",
    "def localpath(path):\n",
    "    return 'file://' + os.path.join(os.path.abspath(os.path.curdir), path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "Posts = sc.textFile(localpath('spark-stats-data/allPosts'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def XmlFilter(x):\n",
    "    if '<row' in x:\n",
    "        try: \n",
    "            etree.fromstring(x)\n",
    "            return True\n",
    "        except:\n",
    "            return False\n",
    "    return False\n",
    "def BadXmls(x):\n",
    "    if '<row' in x:\n",
    "        try:\n",
    "            etree.fromstring(x.encode('utf-8'))\n",
    "            return ('Good', 1)\n",
    "        except:\n",
    "            return ('Bad', 1)\n",
    "    return ('NoTRelev', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CleanPost = Posts.filter(XmlFilter)\n",
    "count = CleanPost.map(lambda x: BadXmls(x))\\\n",
    "             .reduceByKey(lambda x, y: x+y)\n",
    "bad_xml = count.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Favorites and scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're interested in looking for useful patterns in the data.  If we look at the Post data again (the smaller set, `stats.stackexchange.com`), we see that many things about each post are recorded.  We're going to start by looking to see if there is a relationship between the number of times a post was favorited (the `FavoriteCount`) and the `Score`.  The score is the number of times the post was upvoted minus the number of times it was downvoted, so it is a measure of how much a post was liked.  We'd expect posts with a higher number of favorites to have better scores, since they're both measurements of how good the post is.\n",
    "\n",
    "Let's aggregate posts by the number of favorites, and find the average score for each number of favorites. We do this for the lowest 50 numbers of favorites.\n",
    "\n",
    "\n",
    "**Checkpoints**\n",
    "\n",
    "- Total score across all posts: 299469\n",
    "- Mean of first 50 favorite counts (averaging the keys themselves): 24.76"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ZeroNone(x):\n",
    "    if x: return int(x)\n",
    "    else: return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AggregateScore(x):\n",
    "    score = ZeroNone(etree.fromstring(x).get('Score'))\n",
    "    favorite = ZeroNone(etree.fromstring(x).get('FavoriteCount'))\n",
    "    return (favorite, (score, 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "favorite_score = CleanPost.map(lambda x : AggregateScore(x))\\\n",
    "                          .reduceByKey(lambda x,y : (x[0]+y[0], x[1]+y[1]))\\\n",
    "                           .map(lambda x: (x[0], x[1][0]/x[1][1]))\\\n",
    "                            .sortByKey()\n",
    "                          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "favorite_score = favorite_score.take(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer percentage\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we investigate the correlation between a user's reputation and the kind of posts they make. For the 99 users with the highest reputation, we will single out posts which are either questions or answers and look at the percentage of these posts that are answers: *(answers / (answers + questions))*. \n",
    "\n",
    "We only will run this on the statistics overflow set.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checkpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Total questions: 52,060\n",
    "* Total answers: 55,304\n",
    "* Top 99 users' average reputation: 11893.464646464647"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users = sc.textFile(localpath('spark-stats-data/allUsers'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanUsrs = users.filter(XmlFilter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def UserReputation(x):\n",
    "    ID = ZeroNone(etree.fromstring(x).get('Id'))\n",
    "    reput =ZeroNone(etree.fromstring(x).get('Reputation'))\n",
    "    return (ID, reput)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def QuestionsAnswers(x):\n",
    "    ID = ZeroNone(etree.fromstring(x).get('OwnerUserId'))\n",
    "    Ptype = ZeroNone(etree.fromstring(x).get('PostTypeId'))\n",
    "    if Ptype==2:\n",
    "        ans, Q =1, 0\n",
    "    elif Ptype==1:\n",
    "        ans, Q = 0, 1\n",
    "    else:\n",
    "        ans, Q = 0, 0\n",
    "    return (ID, (Q, ans))\n",
    "\n",
    "def AnswerPercentage(x):\n",
    "    if x[1][1]== 0 : return (x[0], 0)\n",
    "    else: return (x[0], x[1][1]/(x[1][0]+x[1][1]))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = CleanPost.map(lambda x: QuestionsAnswers(x))\\\n",
    "                .reduceByKey(lambda x,y: (x[0]+y[0], x[1]+y[1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PostIDQA = CleanPost.map(lambda x: QuestionsAnswers(x))\\\n",
    "                    .reduceByKey(lambda x,y: (x[0]+y[0], x[1]+y[1]))\\\n",
    "                    .map(lambda x: AnswerPercentage(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "UserIDReputation = cleanUsrs.map(lambda x : UserReputation(x))\\\n",
    "                          .sortBy(lambda x: -x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "UserIDReputation = sc.parallelize(UserIDReputation.take(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "USerIDQA = UserIDReputation.join(PostIDQA)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Answer3rd = USerIDQA.map(lambda x: (x[0],x[1][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_percentage = Answer3rd.take(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First question of the user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'd expect the first **question** a user asks to be indicative of their future behavior.  We'll dig more into that in the next problem, but for now let's see the relationship between reputation and how long it took each person to ask their first question.\n",
    "\n",
    "For each user that asked a question, we find the difference between when their account was created (`CreationDate` for the User) and when they asked their first question (`CreationDate` for their first question).  Then we return this time difference in days (round down, so 2.7 days counts as 2 days) for the 100 users with the highest reputation, in the form\n",
    "\n",
    "`(UserId, Days)`\n",
    "\n",
    "**Checkpoints**\n",
    "- Users that asked a question: 23134\n",
    "- Average number of days (round each user's days, then average): 30.1074258"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FilteringQuestions(x):\n",
    "    Ptype = ZeroNone(etree.fromstring(x).get('PostTypeId'))\n",
    "    if Ptype ==1: return True\n",
    "    else: return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Questions = CleanPost.filter(FilteringQuestions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def UserCreationDateReputation(x):\n",
    "    ID = ZeroNone(etree.fromstring(x).get('Id'))\n",
    "    Date = datetime.strptime(etree.fromstring(x).get('CreationDate')[:19],'%Y-%m-%dT%H:%M:%S' )\n",
    "    reput =ZeroNone(etree.fromstring(x).get('Reputation'))\n",
    "    return (ID, (Date, reput))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Users = cleanUsrs.map(lambda x: UserCreationDateReputation(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PostDates(x):\n",
    "    ID = ZeroNone(etree.fromstring(x).get('OwnerUserId'))\n",
    "    QCreationDate = datetime.strptime(etree.fromstring(x).get('CreationDate')[:19],'%Y-%m-%dT%H:%M:%S')\n",
    "    return (ID, QCreationDate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FirstPost = Questions.map(lambda x: PostDates(x))\\\n",
    "                    .reduceByKey(lambda x,y: min(x, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "UserDateandPost = FirstPost.join(Users)\n",
    "First_question = UserDateandPost.map(lambda x: (x[0], (int((x[1][0]-x[1][1][0]).days), x[1][1][1])))\\\n",
    "                                 .sortBy(lambda x: -x[1][1])\n",
    "First_questionF = First_question.map(lambda x: (x[0], x[1][0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_question = First_questionF.take(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identify Active Users (veterans)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be interesting to think about what factors influence a user to remain active on the site over a long period of time. In order not to bias the results towards older users, we'll define a time window between 100 and 150 days after account creation. If the user has made a post in this time, we'll consider them active and well on their way to being veterans of the site; if not, they are inactive and were likely brief users.\n",
    "\n",
    "Let's see if there are differences between the first ever question posts of \"veterans\" vs. \"brief users\". For each group separately, we average the score, views, number of answers, and number of favorites of the users' **first question**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def QuestionInfo(x):\n",
    "    ID = ZeroNone(etree.fromstring(x).get('OwnerUserId'))\n",
    "    ScoreCount = ZeroNone(etree.fromstring(x).get('Score'))\n",
    "    ViewCount = ZeroNone(etree.fromstring(x).get('ViewCount'))\n",
    "    AnswerCount = ZeroNone(etree.fromstring(x).get('AnswerCount'))\n",
    "    FavoriteCount = ZeroNone(etree.fromstring(x).get('FavoriteCount'))\n",
    "    CreationDate = datetime.strptime(etree.fromstring(x).get('CreationDate')[:19],'%Y-%m-%dT%H:%M:%S' )\n",
    "                        \n",
    "    return(ID, (CreationDate,ScoreCount ,ViewCount, AnswerCount, FavoriteCount))\n",
    "\n",
    "def comparison(x,y):\n",
    "    if x[0]<y[0]:\n",
    "        return x\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def UserCreationDate(x):\n",
    "    ID = ZeroNone(etree.fromstring(x).get('Id'))\n",
    "    Date = datetime.strptime(etree.fromstring(x).get('CreationDate')[:19],'%Y-%m-%dT%H:%M:%S' )\n",
    "    return (ID, Date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "UsersIdDate = cleanUsrs.map(lambda x: UserCreationDate(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "QuestionInfo = Questions.map(lambda x: QuestionInfo(x))\\\n",
    "                        .reduceByKey(lambda x, y: comparison(x, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "UserDateDict = {x[0]:x[1] for x in UsersIdDate.collect()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PostsandDates = CleanPost.map(lambda x: PostDates(x))\n",
    "PostsandDates.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def VeteranOrBrief(x):\n",
    "    if int(x[0]) in UserDateDict:\n",
    "        days = (x[1] - UserDateDict[x[0]]).days\n",
    "        if days >= 100 and days <= 150 : V=1\n",
    "        else : V=0\n",
    "    else: V = 0\n",
    "    return (x[0], V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VeteraBriefs = PostsandDates.map(lambda x: VeteranOrBrief(x))\\\n",
    "                            .reduceByKey(lambda x, y: max(x,y))\n",
    "VeteranBriefDict ={x[0]:x[1] for x in VeteraBriefs.collect()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Veteran(x):\n",
    "    if x[0] in VeteranBriefDict:\n",
    "        if VeteranBriefDict[x[0]]==1: return True\n",
    "        else : return False\n",
    "    else: return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def brief(x):\n",
    "    if x[0] in VeteranBriefDict:\n",
    "        if VeteranBriefDict[x[0]]==0: return True\n",
    "        else : return False\n",
    "    else: return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Veterans = UsersIdDate.filter(Veteran)\n",
    "Briefs = UsersIdDate.filter(brief)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VeteransAggregate = Veterans.join(QuestionInfo)\n",
    "BriefsAggregate= Briefs.join(QuestionInfo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checkpoint\n",
    "print(VeteransAggregate.count())\n",
    "print(BriefsAggregate.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Aggregatedreduce(x,y):\n",
    "    return (x[0]+y[0], x[1]+y[1], x[2]+y[2], x[3]+y[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VeteransAggregateF = VeteransAggregate.map(lambda x: ('V', (x[1][1][1], x[1][1][2], x[1][1][3], x[1][1][4])))\\\n",
    "                                     .reduceByKey(lambda x, y: Aggregatedreduce(x,y))\n",
    "BriefsAggregateF = BriefsAggregate.map(lambda x: ('B', (x[1][1][1], x[1][1][2], x[1][1][3], x[1][1][4])))\\\n",
    "                                     .reduceByKey(lambda x, y: Aggregatedreduce(x,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "V = [VeteransAggregateF.collect()[0][1][0], VeteransAggregateF.collect()[0][1][1], VeteransAggregateF.collect()[0][1][2], VeteransAggregateF.collect()[0][1][3]]\n",
    "B = [BriefsAggregateF.collect()[0][1][0], BriefsAggregateF.collect()[0][1][1], BriefsAggregateF.collect()[0][1][2], BriefsAggregateF.collect()[0][1][3]]\n",
    "V1 = [v/1843 for v in V]\n",
    "B1 = [b/21252 for b in B]\n",
    "print(V1)\n",
    "print(B1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Final5V = Veterans.map(lambda x: ('alpha', x))\\\n",
    "                  .reduceByKey(lambda x,y: ReduceTuple(x,y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checkpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Total brief users: 24,864\n",
    "* Total veteran users: 2,027"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "identify_veterans = {\n",
    "    \"vet_score\":V1[0],\n",
    "    \"vet_views\": V1[1],\n",
    "    \"vet_answers\": V1[2],\n",
    "    \"vet_favorites\":V1[3],\n",
    "    \"brief_score\": B1[0],\n",
    "    \"brief_views\": B1[1],\n",
    "    \"brief_answers\": B1[2],\n",
    "    \"brief_favorites\": B1[3]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identify veterans&mdash;full\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same as above, but on the full Stack Exchange data set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def VeteranOrBrief_f(x):\n",
    "    if int(x[0]) in UserDateDict_f:\n",
    "        days = (x[1] - UserDateDict_f[x[0]]).days\n",
    "        if days >= 100 and days <= 150 : V=1\n",
    "        else : V=0\n",
    "    else: V = 0\n",
    "    return (x[0], V)\n",
    "\n",
    "def Veteran_f(x):\n",
    "    if x[0] in VeteranBriefDict_f:\n",
    "        if VeteranBriefDict_f[x[0]]==1: return True\n",
    "        else : return False\n",
    "    else: return False\n",
    "def brief_f(x):\n",
    "    if x[0] in VeteranBriefDict_f:\n",
    "        if VeteranBriefDict_f[x[0]]==0: return True\n",
    "        else : return False\n",
    "    else: return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_f = sc.textFile(localpath('spark-stack-data/allUsers'))\n",
    "post_f = sc.textFile(localpath('spark-stack-data/allPosts'))\n",
    "cleanuser_f = users_f.filter(XmlFilter)\n",
    "cleanpost_f = post_f.filter(XmlFilter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "questions_f = cleanpost_f.filter(FilteringQuestions)\n",
    "UserIdDate_f = cleanuser_f.map(lambda x: UserCreationDate(x))\n",
    "postIdDate_f = cleanpost_f.map(lambda x: PostDates(x))\n",
    "UserDateDict_f = {x[0]:x[1] for x in UserIdDate_f.collect()}\n",
    "VeteraBriefs_f = postIdDate_f.map(lambda x: VeteranOrBrief_f(x))\\\n",
    "                            .reduceByKey(lambda x, y: max(x,y))\n",
    "VeteranBriefDict_f ={x[0]:x[1] for x in VeteraBriefs_f.collect()}\n",
    "Veterans_f = UserIdDate_f.filter(Veteran_f)\n",
    "Briefs_f = UserIdDate_f.filter(brief_f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Veterans Count is '+ str(Veterans_f.count()))\n",
    "print('Briefs Count is '+ str(Briefs_f.count()))\n",
    "QuestionInfo_f = questions_f.map(lambda x: QuestionInfo(x))\\\n",
    "                            .reduceByKey(lambda x, y: comparison(x, y))\n",
    "VeterAggr_f= Veterans_f.join(QuestionInfo_f)\n",
    "BriefsAggr_f = Briefs_f.join(QuestionInfo_f)\n",
    "V_fcount = VeterAggr_f.count()\n",
    "B_fcount = BriefsAggr_f.count()\n",
    "print('Veterans aggregated Count with first question is '+ str(V_fcount))\n",
    "print('Briefs aggregated Count with first questionis '+ str(B_fcount))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VeteransAggregate_F = VeterAggr_f.map(lambda x: ('V', (x[1][1][1], x[1][1][2], x[1][1][3], x[1][1][4])))\\\n",
    "                                     .reduceByKey(lambda x, y: Aggregatedreduce(x,y))\n",
    "BriefsAggregate_F = BriefsAggr_f.map(lambda x: ('B', (x[1][1][1], x[1][1][2], x[1][1][3], x[1][1][4])))\\\n",
    "                                     .reduceByKey(lambda x, y: Aggregatedreduce(x,y))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "V_f = [VeteransAggregate_F.collect()[0][1][0], VeteransAggregate_F.collect()[0][1][1], VeteransAggregate_F.collect()[0][1][2], VeteransAggregate_F.collect()[0][1][3]]\n",
    "B_f = [BriefsAggregate_F.collect()[0][1][0], BriefsAggregate_F.collect()[0][1][1], BriefsAggregate_F.collect()[0][1][2], BriefsAggregate_F.collect()[0][1][3]]\n",
    "V1_f = [v/V_fcount for v in V_f]\n",
    "B1_f = [b/B_fcount for b in B_f]\n",
    "print(V1_f)\n",
    "print(B1_f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checkpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Total brief users: 1,848,628\n",
    "* Total veteran users: 288,285"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cealnpost_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "identify_veterans_full = {\n",
    "    \"vet_score\":V1_f[0],\n",
    "    \"vet_views\": V1_f[1],\n",
    "    \"vet_answers\": V1_f[2],\n",
    "    \"vet_favorites\":V1_f[3],\n",
    "    \"brief_score\": B1_f[0],\n",
    "    \"brief_views\": B1_f[1],\n",
    "    \"brief_answers\": B1_f[2],\n",
    "    \"brief_favorites\": B1_f[3]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2vec\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word2Vec is an alternative approach for vectorizing text data. The vectorized representations of words in the vocabulary tend to be useful for predicting other words in the document, hence the famous example \"vector('king') - vector('man') + vector('woman') ~= vector('queen')\".\n",
    "\n",
    "Let's see how good a Word2Vec model we can train using the tags of each Stack Exchange post as documents (this uses the full data set). We Use the implementation of Word2Vec from pyspark.ml to return a list of the top 25 closest synonyms to \"ggplot2\" and their similarity score in tuple format (\"string\", number).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.feature import HashingTF, Tokenizer\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.ml.feature import Word2Vec\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tagfilter(x):\n",
    "    if etree.fromstring(x.encode('utf-8')).get('Tags'): return True\n",
    "    else: False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tagsOrNot(x):\n",
    "    tags = etree.fromstring(x.encode('utf-8')).get('Tags')\n",
    "    if'<' in tags:\n",
    "        tags = tags.replace('<', ' ')\n",
    "    if '>' in tags:\n",
    "        tags = tags.replace('>', ' ')\n",
    "        tags = tags.split()\n",
    "    return (tags, 1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = CleanPost.filter(tagfilter).map(lambda x: tagsOrNot(x)).map(lambda line: (line.split(\" \"), 1)).toDF(['text', 'score'])\n",
    "test.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ggplotpost = cleanpost_f.filter(tagfilter).map(lambda x: tagsOrNot(x)).toDF(['text', 'score'])\n",
    "w2v = Word2Vec(inputCol=\"text\", outputCol=\"vectors\", vectorSize=100, minCount=15, seed=42)\n",
    "model = w2v.fit(ggplotpost)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans = model.findSynonyms('ggplot2', 25).rdd.take(25)\n",
    "# f = ans.apply(lambda x: (x.__getattr__('word'), x.__getattr__('similarity')))\n",
    "f = []\n",
    "for x in ans:\n",
    "    f. append(((x.__getattr__('word'), x.__getattr__('similarity'))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dimensionality of the vector space should be 100. The random seed should be 42 in `PySpark`, 42L in Scala Spark.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checkpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Mean of the top 25 cosine similarities: 0.8012362027168274"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'd like to see if we can predict the tags of a question from its body text. Instead of predicting specific tags, we will instead try to predict if a question contains one of the top ten most common tags.  \n",
    "\n",
    "To this end, we have separated out a train and a test set from the original data.  The training and tests sets were downloaded with the stats data at the beginning of the notebook.  You can also get them from S3:\n",
    "  * spark-stats-data/posts_train.zip\n",
    "  * spark-stats-data/posts_test.zip\n",
    "\n",
    "This will involve two steps: first, we find the ten most common tags for questions in the training data set (the tags have been removed from the test set). Then train a learner to predict from the text of the question (the `Body` attribute) if it should have one of those ten tags in it - we will process the question text with NLP techniques such as splitting the text into tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checkpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Number of training posts with a tag in the top 10: `19908`\n",
    "- Number without: `17067`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intersection(a,b):\n",
    "    return list(set(a) & set(b))\n",
    "\n",
    "def BodylabelingtagsinTags(x):\n",
    "    tags = etree.fromstring(x.encode('utf-8')).get('Tags')\n",
    "    body = etree.fromstring(x.encode('utf-8')).get('Body')\n",
    "    if'<' in tags:\n",
    "        tags = tags.replace('<', ' ')\n",
    "    if '>' in tags:\n",
    "        tags = tags.replace('>', ' ')\n",
    "        tags = tags.split()\n",
    "    if intersection(tags,toptags):l=1\n",
    "    else: l = 0\n",
    "    return (body, l)\n",
    "\n",
    "def listoftags(x):\n",
    "    tags = etree.fromstring(x.encode('utf-8')).get('Tags')\n",
    "    if'<' in tags:\n",
    "        tags = tags.replace('<', ' ')\n",
    "    if '>' in tags:\n",
    "        tags = tags.replace('>', ' ')\n",
    "        tags = tags.split()\n",
    "    return (tags)\n",
    "\n",
    "def Questionsfilter(x):\n",
    "    Ptype = ZeroNone(etree.fromstring(x).get('PostTypeId'))\n",
    "    if Ptype==1: return True\n",
    "    else: return False\n",
    "    \n",
    "def QuestionsId(x):\n",
    "    return (x, ZeroNone(etree.fromstring(x).get('Id')))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  spark-stats-data/posts_train.zip\n",
      "replace spark-stats-data/train/part-00001? [y]es, [n]o, [A]ll, [N]one, [r]ename: ^C\n",
      "Archive:  spark-stats-data/posts_test.zip\n",
      "replace spark-stats-data/test/part-00001? [y]es, [n]o, [A]ll, [N]one, [r]ename: ^C\n"
     ]
    }
   ],
   "source": [
    "!unzip -d spark-stats-data/train spark-stats-data/posts_train.zip\n",
    "!unzip -d spark-stats-data/test spark-stats-data/posts_test.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "top10tags = sc.textFile(localpath('spark-stats-data/train')).filter(XmlFilter)\\\n",
    "                                                            .filter(tagfilter)\\\n",
    "                                                            .flatMap(lambda tags: listoftags(tags))\\\n",
    "                                                            .map(lambda tag: (tag.lower(), 1))\\\n",
    "                                                            .reduceByKey(lambda x,y: x+y)\\\n",
    "                                                            .sortBy(lambda x: -x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "toptags = [t[0] for t in top10tags.collect()[:10]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['r',\n",
       " 'regression',\n",
       " 'time-series',\n",
       " 'machine-learning',\n",
       " 'probability',\n",
       " 'hypothesis-testing',\n",
       " 'distributions',\n",
       " 'self-study',\n",
       " 'logistic',\n",
       " 'correlation']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toptags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = sc.textFile(localpath('spark-stats-data/train')).filter(XmlFilter)\\\n",
    "                                                          .filter(Questionsfilter)\\\n",
    "                                                          .filter(tagfilter)\\\n",
    "                                                          .map(lambda x: BodylabelingtagsinTags(x))\\\n",
    "                                                          .toDF(['Body', 'label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "testset = sc.textFile(localpath('spark-stats-data/test')).filter(XmlFilter)\\\n",
    "                                                       .filter(Questionsfilter)\\\n",
    "                                                       .map(lambda x: (etree.fromstring(x.encode('utf-8')).get('Body'), ZeroNone(etree.fromstring(x).get('Id'))) )\\\n",
    "                                                       .sortBy(lambda x: x[1])\\\n",
    "                                                       .toDF(['Body', 'ID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain = []\n",
    "Y = []\n",
    "for t in trainset.collect():\n",
    "    Xtrain.append(t.__getattr__('Body'))\n",
    "    Y.append(t.__getattr__('label'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtest=[]\n",
    "for t in testset.collect():\n",
    "    Xtest.append(t.__getattr__('Body'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4649"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <object repr() failed>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/pyspark/ml/wrapper.py\", line 40, in __del__\n",
      "    if SparkContext._active_spark_context and self._java_obj is not None:\n",
      "AttributeError: 'LogisticRegression' object has no attribute '_java_obj'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('HashingVect', HashingVectorizer(alternate_sign=True, analyzer='word', binary=False,\n",
       "         decode_error='strict', dtype=<class 'numpy.float64'>,\n",
       "         encoding='utf-8', input='content', lowercase=True,\n",
       "         n_features=1048576, ngram_range=(1, 1), non_negative=False,\n",
       "         norm='...penalty='l2', random_state=123, solver='lbfgs', tol=0.0001,\n",
       "          verbose=0, warm_start=False))])"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LG = LogisticRegression(random_state=123, solver='lbfgs')\n",
    "\n",
    "bag_of_words_est = Pipeline([\n",
    "    (\"HashingVect\", HashingVectorizer()),\n",
    "    (\"LG\", LG)\n",
    "])\n",
    "bag_of_words_est.fit(Xtrain, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = bag_of_words_est.predict(Xtest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Copyright &copy; 2019 The Data Incubator.  All rights reserved.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "nbclean": true
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
